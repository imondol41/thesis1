{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_optimized_detr.py\n",
    "import os, time, math, yaml\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ============ GPU CHECK ============\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç SYSTEM CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"‚úÖ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA NOT AVAILABLE - Training will be VERY slow on CPU!\")\n",
    "    print(\"Please install: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\")\n",
    "    raise RuntimeError(\"CUDA required for training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "DATA_YAML = r\"F:\\skills-copilot-codespaces-vscode\\thesis\\rsuddataset\\rsud20k\\images\\data_fixed.yaml\"\n",
    "TRAIN_IMG_DIR = r\"F:\\skills-copilot-codespaces-vscode\\thesis\\rsuddataset\\rsud20k\\images\\train\"\n",
    "TRAIN_LABEL_DIR = r\"F:\\skills-copilot-codespaces-vscode\\thesis\\rsuddataset\\rsud20k\\labels\\train\"\n",
    "CACHE_DIR = r\"F:\\skills-copilot-codespaces-vscode\\thesis\\cache\"\n",
    "DEVICE = \"cuda\"\n",
    "IMAGE_SIZE = 640\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "NUM_EPOCHS = 1\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "USE_CACHE = False\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "# ------------------------------------------\n",
    "\n",
    "# ---------- Load YAML ----------\n",
    "with open(DATA_YAML, \"r\") as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "NUM_CLASSES = data_cfg.get(\"nc\", None)\n",
    "if NUM_CLASSES is None:\n",
    "    raise RuntimeError(\"nc not found in YAML\")\n",
    "\n",
    "print(f\"‚úÖ Using device: {DEVICE} | classes: {NUM_CLASSES}\")\n",
    "\n",
    "# ---------- Transform ----------\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ---------- Dataset with EXTREME validation ----------\n",
    "class RSUDDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, processor, num_classes, transform=None, use_cache=False, cache_dir=\"cache\"):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "        self.use_cache = use_cache\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Get all images\n",
    "        all_image_files = sorted([p.name for p in self.image_dir.iterdir() if p.suffix.lower() in [\".jpg\",\".png\",\".jpeg\"]])\n",
    "        self.image_files = []\n",
    "        self.valid_box_counts = {}\n",
    "        skipped = 0\n",
    "        invalid_classes = set()\n",
    "        \n",
    "        print(f\"üîç Performing EXTREME validation (num_classes={num_classes})...\")\n",
    "        for img_name in tqdm(all_image_files, desc=\"Scanning labels\"):\n",
    "            label_path = self.label_dir / (Path(img_name).stem + \".txt\")\n",
    "            \n",
    "            if not label_path.exists():\n",
    "                skipped += 1\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                content = label_path.read_text().strip()\n",
    "                if not content:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                    \n",
    "                lines = content.splitlines()\n",
    "                valid_boxes = 0\n",
    "                \n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if not line or line.startswith('#'):\n",
    "                        continue\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 5:\n",
    "                        try:\n",
    "                            cls = int(parts[0])\n",
    "                            xc, yc, w, h = map(float, parts[1:5])\n",
    "                            \n",
    "                            # üî• CRITICAL: Validate class ID range\n",
    "                            if cls < 0 or cls >= num_classes:\n",
    "                                invalid_classes.add(cls)\n",
    "                                continue\n",
    "                            \n",
    "                            # Validate coordinates\n",
    "                            if 0 <= xc <= 1 and 0 <= yc <= 1 and 0 < w <= 1 and 0 < h <= 1:\n",
    "                                valid_boxes += 1\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                if valid_boxes > 0:\n",
    "                    self.image_files.append(img_name)\n",
    "                    self.valid_box_counts[img_name] = valid_boxes\n",
    "                else:\n",
    "                    skipped += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                skipped += 1\n",
    "        \n",
    "        if invalid_classes:\n",
    "            print(f\"‚ö†Ô∏è  WARNING: Found invalid class IDs: {sorted(invalid_classes)} (valid range: 0-{num_classes-1})\")\n",
    "        \n",
    "        print(f\"‚úì Dataset validated: {len(self.image_files)} images (skipped {skipped})\")\n",
    "        print(f\"‚úì Total boxes: {sum(self.valid_box_counts.values())}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = self.image_dir / img_name\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        img_w, img_h = image.size\n",
    "\n",
    "        # Parse labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        label_path = self.label_dir / (Path(img_name).stem + \".txt\")\n",
    "        \n",
    "        for line in label_path.read_text().splitlines():\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 5:\n",
    "                try:\n",
    "                    cls = int(parts[0])\n",
    "                    \n",
    "                    # üî• CRITICAL: Skip invalid class IDs\n",
    "                    if cls < 0 or cls >= self.num_classes:\n",
    "                        continue\n",
    "                    \n",
    "                    xc, yc, w, h = map(float, parts[1:5])\n",
    "                    \n",
    "                    # Validate and convert\n",
    "                    if 0 <= xc <= 1 and 0 <= yc <= 1 and 0 < w <= 1 and 0 < h <= 1:\n",
    "                        x_min = max(0, (xc - w/2) * img_w)\n",
    "                        y_min = max(0, (yc - h/2) * img_h)\n",
    "                        x_max = min(img_w, (xc + w/2) * img_w)\n",
    "                        y_max = min(img_h, (yc + h/2) * img_h)\n",
    "                        \n",
    "                        # Ensure box has area\n",
    "                        if x_max > x_min + 1 and y_max > y_min + 1:  # At least 1px area\n",
    "                            boxes.append([x_min, y_min, x_max, y_max])\n",
    "                            labels.append(cls)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        # Transform image\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(image)\n",
    "        else:\n",
    "            img_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "        # üî• FINAL SAFETY CHECK\n",
    "        if len(boxes) == 0:\n",
    "            print(f\"\\n‚ùå ERROR: {img_name} has 0 valid boxes!\")\n",
    "            print(f\"   Expected: {self.valid_box_counts.get(img_name, 'unknown')}\")\n",
    "            print(f\"   Label file: {label_path}\")\n",
    "            print(f\"   Content preview: {label_path.read_text()[:300]}\")\n",
    "            raise RuntimeError(f\"Image {img_name} has no valid boxes after filtering!\")\n",
    "        \n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"class_labels\": torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return (img_tensor, target)\n",
    "\n",
    "# ---------- Collate function ----------\n",
    "def collate_fn(batch):\n",
    "    imgs = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    \n",
    "    # Validate each target\n",
    "    for i, t in enumerate(targets):\n",
    "        if t[\"boxes\"].shape[0] == 0:\n",
    "            print(f\"\\n‚ùå BATCH ERROR: Item {i} has 0 boxes!\")\n",
    "            raise RuntimeError(\"Empty boxes in batch!\")\n",
    "        if t[\"class_labels\"].max() >= NUM_CLASSES:\n",
    "            print(f\"\\n‚ùå BATCH ERROR: Item {i} has invalid class ID {t['class_labels'].max()} (max allowed: {NUM_CLASSES-1})\")\n",
    "            raise RuntimeError(\"Invalid class ID in batch!\")\n",
    "    \n",
    "    return imgs, targets\n",
    "\n",
    "# ---------- Create dataset ----------\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "train_ds = RSUDDataset(\n",
    "    TRAIN_IMG_DIR, \n",
    "    TRAIN_LABEL_DIR, \n",
    "    processor, \n",
    "    num_classes=NUM_CLASSES,\n",
    "    transform=simple_transform, \n",
    "    use_cache=USE_CACHE, \n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# ---------- Load model ----------\n",
    "print(\"üì• Loading DETR model...\")\n",
    "model = DetrForObjectDetection.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\", \n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded | Classifier expects {NUM_CLASSES} classes\")\n",
    "print(f\"   Classifier head: {model.class_labels_classifier}\")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ---------- Optimizer & Scaler ----------\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = GradScaler()\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"üîπ cuDNN benchmark enabled\")\n",
    "print(\"üîπ Mixed precision (FP16) enabled\")\n",
    "print(f\"üîπ Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "\n",
    "# ---------- TRAINING ----------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", ncols=120)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, (imgs, targets) in pbar:\n",
    "        # Stack images\n",
    "        pixel_values = torch.stack(imgs).to(DEVICE, non_blocking=True)\n",
    "\n",
    "        # Prepare targets with validation\n",
    "        tgt_for_model = []\n",
    "        for i, t in enumerate(targets):\n",
    "            num_boxes = t[\"boxes\"].shape[0]\n",
    "            num_labels = t[\"class_labels\"].shape[0]\n",
    "            max_class = t[\"class_labels\"].max().item()\n",
    "            \n",
    "            # üî• EXTREME validation\n",
    "            if num_boxes == 0:\n",
    "                print(f\"\\n‚ùå Step {step}, sample {i}: 0 boxes!\")\n",
    "                raise RuntimeError(\"Empty boxes!\")\n",
    "            if num_boxes != num_labels:\n",
    "                print(f\"\\n‚ùå Step {step}, sample {i}: boxes({num_boxes}) != labels({num_labels})\")\n",
    "                raise RuntimeError(\"Mismatched boxes/labels!\")\n",
    "            if max_class >= NUM_CLASSES:\n",
    "                print(f\"\\n‚ùå Step {step}, sample {i}: class {max_class} >= {NUM_CLASSES}\")\n",
    "                raise RuntimeError(\"Invalid class ID!\")\n",
    "                \n",
    "            tgt_for_model.append({\n",
    "                \"boxes\": t[\"boxes\"].to(DEVICE, non_blocking=True),\n",
    "                \"class_labels\": t[\"class_labels\"].to(DEVICE, non_blocking=True)\n",
    "            })\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            outputs = model(pixel_values=pixel_values, labels=tgt_for_model)\n",
    "            loss = outputs.loss / GRAD_ACCUM_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            gpu_mem = torch.cuda.memory_allocated(0) / 1e9\n",
    "            pbar.set_postfix(loss=f\"{(epoch_loss / (step+1)):.4f}\", gpu=f\"{gpu_mem:.1f}GB\")\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} complete | avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt_path = os.path.join(CHECKPOINT_DIR, f\"detr_epoch{epoch+1}.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "    }, ckpt_path)\n",
    "    print(f\"üíæ Saved: {ckpt_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ TRAINING FINISHED!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f694d32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cuda\n",
      "‚úÖ cuDNN benchmark + TF32 enabled\n",
      "‚úÖ Classes detected: ['person', 'rickshaw', 'rickshaw_van', 'auto_rickshaw', 'truck', 'pickup_truck', 'private_car', 'motorcycle', 'bicycle', 'bus', 'micro_bus', 'covered_van', 'human_hauler']\n",
      "‚úÖ Number of classes: 13\n",
      "\n",
      "============================================================\n",
      "üî• LOADING ALL IMAGES TO RAM (2-3 minutes)\n",
      "============================================================\n",
      "‚úÖ Found 18681 images\n",
      "üîÑ Loading 18681 images to RAM (this will take ~2 min)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HACKER\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\backends\\__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
      "  self.setter(val)\n",
      "Caching to RAM:  25%|‚ñà‚ñà‚ñç       | 4661/18681 [01:20<04:00, 58.19it/s] \n",
      "Caching to RAM:  25%|‚ñà‚ñà‚ñç       | 4661/18681 [01:20<04:00, 58.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müî• LOADING ALL IMAGES TO RAM (2-3 minutes)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m train_dataset = \u001b[43mFastImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_images\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# üî• Cache ALL\u001b[39;00m\n\u001b[32m    123\u001b[39m val_dataset = FastImageDataset(VAL_DIR, transform, cache_images=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    125\u001b[39m train_loader = DataLoader(\n\u001b[32m    126\u001b[39m     train_dataset, \n\u001b[32m    127\u001b[39m     batch_size=BATCH_SIZE, \n\u001b[32m   (...)\u001b[39m\u001b[32m    130\u001b[39m     pin_memory=PIN_MEMORY\n\u001b[32m    131\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mFastImageDataset.__init__\u001b[39m\u001b[34m(self, root_dir, transform, cache_images)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# üî• Use 8 threads to load images in parallel\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers=\u001b[32m8\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCaching to RAM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, img \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache[idx] = img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HACKER\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HACKER\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\concurrent\\futures\\_base.py:639\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HACKER\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\concurrent\\futures\\_base.py:311\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HACKER\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\concurrent\\futures\\_base.py:445\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    448\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HACKER\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\threading.py:369\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================================\n",
    "# üîπ 1. Configuration (MAXIMUM GPU BOOST)\n",
    "# =========================================\n",
    "TRAIN_DIR = r\"F:\\skills-copilot-codespaces-vscode\\thesis\\rsuddataset\\rsud20k\\images\\train\"\n",
    "VAL_DIR = r\"F:\\skills-copilot-codespaces-vscode\\thesis\\rsuddataset\\rsud20k\\images\\val\"\n",
    "LABELS_FILE = r\"F:\\skills-copilot-codespaces-vscode\\thesis\\rsuddataset\\rsud20k\\images\\data_fixed.yaml\"\n",
    "SAVE_PATH = r\"F:\\skills-copilot-codespaces-vscode\\thesis\\best_dinov2_model.pt\"\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 96          # üî• MAXIMIZE batch size (12GB VRAM = can handle 96)\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 0          # üî• 0 workers - Windows overhead too high, rely on GPU parallelism\n",
    "PIN_MEMORY = True        # üî• Faster CPU‚ÜíGPU transfer\n",
    "ACCUMULATION_STEPS = 2   # üî• Gradient accumulation for effective batch=192\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# üî• Enable ALL GPU optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # üî• TF32 for RTX 30-series\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "print(\"‚úÖ cuDNN benchmark + TF32 enabled\")\n",
    "\n",
    "# =========================================\n",
    "# üîπ 2. Load YAML labels\n",
    "# =========================================\n",
    "with open(LABELS_FILE, \"r\") as f:\n",
    "    labels_yaml = yaml.safe_load(f)\n",
    "\n",
    "CLASSES = labels_yaml[\"names\"] if \"names\" in labels_yaml else labels_yaml\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(\"‚úÖ Classes detected:\", CLASSES)\n",
    "print(f\"‚úÖ Number of classes: {NUM_CLASSES}\")\n",
    "\n",
    "# =========================================\n",
    "# üîπ 3. Fast Dataset with AGGRESSIVE RAM CACHING\n",
    "# =========================================\n",
    "class FastImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, cache_images=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = [\n",
    "            os.path.join(root_dir, f)\n",
    "            for f in os.listdir(root_dir)\n",
    "            if f.endswith(('.jpg', '.png', '.jpeg'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "        self.cache = {}\n",
    "        print(f\"‚úÖ Found {len(self.image_files)} images\")\n",
    "        \n",
    "        # üî• ALWAYS cache images to RAM (eliminate I/O bottleneck)\n",
    "        if cache_images:\n",
    "            print(f\"üîÑ Loading {len(self.image_files)} images to RAM (this will take ~2 min)...\")\n",
    "            from concurrent.futures import ThreadPoolExecutor\n",
    "            \n",
    "            def load_image(idx_path):\n",
    "                idx, img_path = idx_path\n",
    "                return idx, Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # üî• Use 8 threads to load images in parallel\n",
    "            with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "                results = list(tqdm(\n",
    "                    executor.map(load_image, enumerate(self.image_files)),\n",
    "                    total=len(self.image_files),\n",
    "                    desc=\"Caching to RAM\"\n",
    "                ))\n",
    "            \n",
    "            for idx, img in results:\n",
    "                self.cache[idx] = img\n",
    "            \n",
    "            print(f\"‚úÖ All {len(self.cache)} images cached in RAM!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # üî• Always use cached image (no disk I/O during training)\n",
    "        if idx in self.cache:\n",
    "            image = self.cache[idx].copy()  # Copy to avoid mutation\n",
    "        else:\n",
    "            img_path = self.image_files[idx]\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        label = idx % NUM_CLASSES\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# =========================================\n",
    "# üîπ 4. Data Augmentation (GPU-Accelerated)\n",
    "# =========================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), antialias=True),  # üî• GPU-accelerated resize\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "# =========================================\n",
    "# üîπ MAIN EXECUTION (Windows multiprocessing fix)\n",
    "# =========================================\n",
    "if __name__ == '__main__':\n",
    "    # =========================================\n",
    "    # üîπ 5. Datasets & Loaders (ALL DATA IN RAM)\n",
    "    # =========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî• LOADING ALL IMAGES TO RAM (2-3 minutes)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    train_dataset = FastImageDataset(TRAIN_DIR, transform, cache_images=True)  # üî• Cache ALL\n",
    "    val_dataset = FastImageDataset(VAL_DIR, transform, cache_images=True)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY\n",
    "    )\n",
    "\n",
    "    print(f\"üìÅ Train: {len(train_dataset)} images | Validation: {len(val_dataset)} images\")\n",
    "    print(f\"üî• Batch size: {BATCH_SIZE} | Effective batch (with accumulation): {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
    "\n",
    "    # =========================================\n",
    "    # üîπ 6. Model Setup (DINOv2 / ResNet fallback)\n",
    "    # =========================================\n",
    "    try:\n",
    "        dino = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14', pretrained=True)\n",
    "        print(\"‚úÖ Loaded DINOv2 backbone.\")\n",
    "        \n",
    "        # Freeze feature extractor\n",
    "        for param in dino.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # DINOv2: head is Identity, replace with Linear (vits14 = 384 dims)\n",
    "        dino.head = nn.Linear(384, NUM_CLASSES)\n",
    "        print(\"‚úÖ Replaced DINOv2 head with classifier\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è DINOv2 not found: {e}\")\n",
    "        print(\"‚ö†Ô∏è Using ResNet50 instead.\")\n",
    "        dino = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Freeze feature extractor\n",
    "        for param in dino.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # ResNet fallback\n",
    "        dino.fc = nn.Linear(dino.fc.in_features, NUM_CLASSES)\n",
    "        print(\"‚úÖ Replaced ResNet classifier\")\n",
    "\n",
    "    model = dino.to(device)\n",
    "    \n",
    "    # üî• Compile model for 20-30% speedup (PyTorch 2.0+)\n",
    "    try:\n",
    "        model = torch.compile(model, mode='max-autotune')\n",
    "        print(\"‚úÖ Model compiled with torch.compile (max-autotune)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è torch.compile not available: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Model on {device}\")\n",
    "    \n",
    "    # =========================================\n",
    "    # üîπ 7. Loss, Optimizer + Mixed Precision\n",
    "    # =========================================\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)  # üî• AdamW\n",
    "\n",
    "    # üî• Mixed precision training (FP16) - 2x faster, uses less memory\n",
    "    scaler = GradScaler()\n",
    "    print(\"‚úÖ Mixed precision (FP16) + AdamW optimizer enabled\")\n",
    "    \n",
    "    # =========================================\n",
    "    # üîπ 8. Train + Validate Loop (GPU OPTIMIZED)\n",
    "    # =========================================\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ STARTING TRAINING (MAXIMUM GPU ACCELERATION)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # üî• Training with mixed precision + gradient accumulation\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"üåÄ Epoch {epoch+1}/{EPOCHS}\", ncols=100)):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # üî• Mixed precision forward pass (FP16)\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / ACCUMULATION_STEPS  # üî• Scale loss for accumulation\n",
    "            \n",
    "            # üî• Mixed precision backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # üî• Only update weights every ACCUMULATION_STEPS\n",
    "            if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            running_loss += loss.item() * ACCUMULATION_STEPS  # Unscale for logging\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "\n",
    "        # ===================== Validation =====================\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                \n",
    "                # üî• Use FP16 in validation too\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_acc_list.append(val_accuracy)\n",
    "\n",
    "        # üî• Show GPU memory usage\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "        print(f\"üìâ Train Loss: {avg_train_loss:.4f} | üßæ Val Loss: {avg_val_loss:.4f} | üéØ Val Acc: {val_accuracy:.2f}% | üíæ GPU: {gpu_mem:.2f}GB\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "            print(f\"‚úÖ Model saved at {SAVE_PATH} (Best Val Loss: {best_val_loss:.4f})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ TRAINING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # =========================================\n",
    "    # üîπ 9. Plot Graphs\n",
    "    # =========================================\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(train_loss_list, label='Train Loss', color='blue')\n",
    "    plt.plot(val_loss_list, label='Val Loss', color='red')\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(val_acc_list, label='Validation Accuracy', color='green')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"accuracy_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ Training Complete! Best model saved at:\", SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ca207d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcsv\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Video inference (batched frames) ‚Äî paste your video path below then run this cell.\n",
    "# Outputs:\n",
    "#  - Annotated video: runs/detect/video_test/<video>_annotated.mp4\n",
    "#  - CSV detections: runs/detect/video_test/<video>_detections.csv\n",
    "#  - Prints class summary and displays first annotated frame inline\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ------------------- USER CONFIG -------------------\n",
    "VIDEO_PATH = Path(r\"F:\\skills-copilot-codespaces-vscode\\PXL_20250507_113206344.TS.mp4\")  # <-- set this to your video\n",
    "OUTPUT_DIR = Path(\"runs/detect/video_test\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMGSZ = 1280     # model input size (reduce if OOM)\n",
    "CONF = 0.25\n",
    "IOU = 0.45\n",
    "FRAME_BATCH = 8  # process frames in batches for higher throughput\n",
    "# ---------------------------------------------------\n",
    "\n",
    "OUTPUT_VIDEO = OUTPUT_DIR / (VIDEO_PATH.stem + \"_annotated.mp4\")\n",
    "DETECTIONS_CSV = OUTPUT_DIR / (VIDEO_PATH.stem + \"_detections.csv\")\n",
    "\n",
    "# ensure model available in notebook; otherwise load fallback\n",
    "try:\n",
    "    model\n",
    "    print(\"Reusing `model` from notebook globals\")\n",
    "except NameError:\n",
    "    TRAINED_MODEL = Path(\"runs/detect/rsud20k_yolo11/weights/best.pt\")\n",
    "    MODEL_PATH = str(TRAINED_MODEL) if TRAINED_MODEL.exists() else \"yolo11x.pt\"\n",
    "    print(f\"Loading YOLO model: {MODEL_PATH}\")\n",
    "    model = YOLO(MODEL_PATH)\n",
    "\n",
    "# set thresholds\n",
    "try:\n",
    "    model.conf = CONF\n",
    "    model.iou = IOU\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# open video\n",
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Cannot open video: {VIDEO_PATH}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "writer = cv2.VideoWriter(str(OUTPUT_VIDEO), fourcc, fps, (w, h))\n",
    "\n",
    "print(f\"Processing video: {VIDEO_PATH} -> {OUTPUT_VIDEO} | fps={fps}, size=({w},{h})\")\n",
    "\n",
    "frame_idx = 0\n",
    "summary_counts = {}\n",
    "per_frame_rows = []  # (frame_idx, class_name, conf, x1,y1,x2,y2)\n",
    "\n",
    "# read frames and inference in batches\n",
    "batch_frames = []\n",
    "batch_frame_indices = []\n",
    "\n",
    "pbar = tqdm(total=int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0), desc=\"Frames\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_idx += 1\n",
    "    batch_frames.append(frame.copy())\n",
    "    batch_frame_indices.append(frame_idx)\n",
    "\n",
    "    if len(batch_frames) == FRAME_BATCH:\n",
    "        # run batched inference\n",
    "        # ultralytics supports lists/arrays as source\n",
    "        results = model.predict(source=batch_frames, imgsz=IMGSZ, conf=CONF, iou=IOU, verbose=False)\n",
    "        # results is list of results for each input\n",
    "        for i, res in enumerate(results):\n",
    "            cur_frame = batch_frames[i]\n",
    "            cur_idx = batch_frame_indices[i]\n",
    "            if hasattr(res, 'boxes') and res.boxes is not None and len(res.boxes) > 0:\n",
    "                for box in res.boxes:\n",
    "                    # handle both torch tensors or numpy\n",
    "                    xyxy = box.xyxy[0].cpu().numpy() if hasattr(box.xyxy, 'cpu') else box.xyxy[0].numpy()\n",
    "                    conf_val = float(box.conf[0].cpu()) if hasattr(box.conf, 'cpu') else float(box.conf[0])\n",
    "                    cls_id = int(box.cls[0].cpu()) if hasattr(box.cls, 'cpu') else int(box.cls[0])\n",
    "                    cls_name = model.names[cls_id] if cls_id in model.names else str(cls_id)\n",
    "\n",
    "                    x1, y1, x2, y2 = map(int, xyxy)\n",
    "                    cv2.rectangle(cur_frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "                    label = f\"{cls_name} {conf_val:.2f}\"\n",
    "                    cv2.putText(cur_frame, label, (x1, max(15, y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "\n",
    "                    summary_counts[cls_name] = summary_counts.get(cls_name, 0) + 1\n",
    "                    per_frame_rows.append([cur_idx, cls_name, conf_val, x1, y1, x2, y2])\n",
    "\n",
    "            # write annotated frame\n",
    "            writer.write(cur_frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "        # clear batch\n",
    "        batch_frames = []\n",
    "        batch_frame_indices = []\n",
    "\n",
    "# handle leftover frames\n",
    "if len(batch_frames) > 0:\n",
    "    results = model.predict(source=batch_frames, imgsz=IMGSZ, conf=CONF, iou=IOU, verbose=False)\n",
    "    for i, res in enumerate(results):\n",
    "        cur_frame = batch_frames[i]\n",
    "        cur_idx = batch_frame_indices[i]\n",
    "        if hasattr(res, 'boxes') and res.boxes is not None and len(res.boxes) > 0:\n",
    "            for box in res.boxes:\n",
    "                xyxy = box.xyxy[0].cpu().numpy() if hasattr(box.xyxy, 'cpu') else box.xyxy[0].numpy()\n",
    "                conf_val = float(box.conf[0].cpu()) if hasattr(box.conf, 'cpu') else float(box.conf[0])\n",
    "                cls_id = int(box.cls[0].cpu()) if hasattr(box.cls, 'cpu') else int(box.cls[0])\n",
    "                cls_name = model.names[cls_id] if cls_id in model.names else str(cls_id)\n",
    "\n",
    "                x1, y1, x2, y2 = map(int, xyxy)\n",
    "                cv2.rectangle(cur_frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "                label = f\"{cls_name} {conf_val:.2f}\"\n",
    "                cv2.putText(cur_frame, label, (x1, max(15, y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "\n",
    "                summary_counts[cls_name] = summary_counts.get(cls_name, 0) + 1\n",
    "                per_frame_rows.append([cur_idx, cls_name, conf_val, x1, y1, x2, y2])\n",
    "        writer.write(cur_frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "# write CSV\n",
    "with open(DETECTIONS_CSV, 'w', newline='') as cf:\n",
    "    writer_csv = csv.writer(cf)\n",
    "    writer_csv.writerow(['frame_idx', 'class', 'conf', 'x1', 'y1', 'x2', 'y2'])\n",
    "    writer_csv.writerows(per_frame_rows)\n",
    "\n",
    "print(f\"Done. Annotated video saved: {OUTPUT_VIDEO}\")\n",
    "print(f\"Detections CSV saved: {DETECTIONS_CSV}\")\n",
    "print(\"Summary counts:\")\n",
    "for cls, cnt in sorted(summary_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {cls}: {cnt}\")\n",
    "\n",
    "# display first frame for quick preview (if in notebook)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    import PIL.Image\n",
    "    cap2 = cv2.VideoCapture(str(OUTPUT_VIDEO))\n",
    "    ret, f0 = cap2.read()\n",
    "    cap2.release()\n",
    "    if ret:\n",
    "        f0 = cv2.cvtColor(f0, cv2.COLOR_BGR2RGB)\n",
    "        display(PIL.Image.fromarray(f0))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
